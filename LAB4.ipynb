{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be430cc-d101-457b-8041-ea001c3844a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account_name = \"goodreadsreviews60313569\"\n",
    "account_key=\"TLLUqPCW94aCOPP/8ic0S/0oWOEa9ccZ/rfHpUw71lh9c2vsCCl2oFGaWifNoAmtRTC45MDRTn0B+AStXqUGDQ==\"  # the long key from the portal / lab\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d0544b-710d-49af-ad1a-744ae46a36ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_df = spark.read.format(\"delta\").load(\n",
    "    \"abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/feature_v2/train\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2a0e09-942f-4cdf-b5ca-cbb5b5c6ba2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (3.9.2)\nRequirement already satisfied: click in /databricks/python3/lib/python3.12/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.12/site-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from nltk) (2025.11.3)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from nltk) (4.67.1)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "\n",
    "import nltk\n",
    "# Other code that uses nltk follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d8e3ea7-a559-4f57-a977-91be01f980e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (4.57.1)\nRequirement already satisfied: torch in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (2.9.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.15.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.12/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from transformers) (2.32.2)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /databricks/python3/lib/python3.12/site-packages (from torch) (4.11.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (74.0.0)\nRequirement already satisfied: sympy>=1.13.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (12.8.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (11.3.3.83)\nRequirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (10.3.9.90)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (11.7.3.90)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (12.5.8.93)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (12.8.90)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (12.8.93)\nRequirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (1.13.1.3)\nRequirement already satisfied: triton==3.5.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from torch) (3.5.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face Transformers and Torch.\n",
    "# IMPORTANT: After running this cell, restart your cluster or re-run all cells to ensure the libraries are available before any import statements.\n",
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207d6d1b-ddf3-4603-9f1b-7cdc90c7371d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e26db6c5-ed21-4f5e-bbc3-b5d4c0cca150/lib/python3.12/site-packages (2.15.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nHello, world! \n"
     ]
    }
   ],
   "source": [
    "%pip install emoji\n",
    "\n",
    "import emoji\n",
    "\n",
    "# Example usage of the emoji library\n",
    "text_with_emoji = \"Hello, world! \uD83C\uDF0D\"\n",
    "cleaned_text = emoji.replace_emoji(text_with_emoji, replace='')  # Remove emojis\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd6b7c8-c361-4d7e-8ba6-6dfa5998f59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: goodreadsreviews60313569: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "az storage container create \\\n",
    "  --name lakehouse \\\n",
    "  --account-name <goodreadsreviews60313569> \\\n",
    "  --auth-mode login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004e3ec4-274f-4f49-8d14-86d0a5d6c576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not read Parquet path abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/feature_v2/. Creating empty DataFrame with schema. Error: [NOT_IMPLEMENTED] rdd is not implemented.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import os\n",
    "\n",
    "# TODO: Update the schema below to match your actual data columns and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"book_id\", StringType(), True),\n",
    "    # Add more fields as needed\n",
    "])\n",
    "\n",
    "parquet_path = 'abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/feature_v2/'\n",
    "\n",
    "try:\n",
    "    df = spark.read.schema(schema).parquet(parquet_path)\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"WARNING: Parquet path {parquet_path} is empty. Creating empty DataFrame with schema.\")\n",
    "        df = spark.createDataFrame([], schema)\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Could not read Parquet path {parquet_path}. Creating empty DataFrame with schema. Error: {e}\")\n",
    "    df = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f102008-258d-4ba1-922e-bfa7e3714de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/feature_v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92980072-aec9-4cb5-b48b-81131ba995ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storage_account = goodreadsreviews60313569\nbase_path      = abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/feature_v2\n"
     ]
    }
   ],
   "source": [
    "storage_account = \"goodreadsreviews60313569\"\n",
    "container = \"lakehouse\"\n",
    "\n",
    "account_key = \"TLLUqPCW94aCOPP/8ic0S/0oWOEa9ccZ/rfHpUw71lh9c2vsCCl2oFGaWifNoAmtRTC45MDRTn0B+AStXqUGDQ==\"  # example format\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n",
    "base_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/gold/feature_v2\"\n",
    "\n",
    "print(\"storage_account =\", storage_account)\n",
    "print(\"base_path      =\", base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6a11d0-56cd-4c1c-b729-71ca2d55f6b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0824,  0.0667, -0.2880,  ..., -0.3566,  0.1960,  0.5381],\n         [ 0.0310, -0.1448,  0.0952,  ..., -0.1560,  1.0151,  0.0947],\n         [-0.8935,  0.3240,  0.4184,  ..., -0.5498,  0.2853,  0.1149],\n         ...,\n         [-0.2812, -0.8531,  0.6912,  ..., -0.5051,  0.4716, -0.6854],\n         [-0.4429, -0.7820, -0.8055,  ...,  0.1949,  0.1081,  0.0130],\n         [ 0.5570, -0.1080, -0.2412,  ...,  0.2817, -0.3996, -0.1882]]])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "try:\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    import torch\n",
    "except ModuleNotFoundError as e:\n",
    "    raise ModuleNotFoundError(\n",
    "        \"transformers or torch is not installed. Please run the cell with '%pip install transformers torch' at the top of the notebook, then restart your cluster or re-run all cells.\"\n",
    "    ) from e\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "except Exception as e:\n",
    "    print(\"Error loading BERT model or tokenizer:\", e)\n",
    "    raise\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embeddings(text):\n",
    "    # Encode the text\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Return the last hidden state\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "embeddings = get_bert_embeddings(text)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e798939f-4a76-462a-884a-471b417c0994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assume df is your Spark DataFrame\n",
    "splits = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "train_df = splits[0]\n",
    "validation_df = splits[1]\n",
    "test_df = splits[2]\n",
    "# Now you can write to ADLS using the same code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d688ed64-fd2b-4e32-a739-dea50825251b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The following code is commented out because the path may be empty or missing and would cause schema inference errors.\n",
    "# df = spark.read.parquet(\n",
    "#   \"abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/feature_v2\"\n",
    "# )\n",
    "# Using df loaded from gold/feature_v2/ instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f37de1a4-f6d1-4aa4-8c0d-ee18e3282ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: DataFrame is empty. Skipping split and write steps.\n"
     ]
    }
   ],
   "source": [
    "# Use the DataFrame loaded from gold/feature_v2/\n",
    "# (Assume df is already loaded in cell 3)\n",
    "\n",
    "if not df.head(1):\n",
    "    print(\"WARNING: DataFrame is empty. Skipping split and write steps.\")\n",
    "else:\n",
    "    # Split the DataFrame\n",
    "    total = df.count()\n",
    "    train_df, validation_df, test_df = df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
    "\n",
    "    # Save each split to its respective subfolder in the gold layer using base_path\n",
    "    train_df.write.mode(\"overwrite\").parquet(f\"{base_path}/train\")\n",
    "    validation_df.write.mode(\"overwrite\").parquet(f\"{base_path}/validation\")\n",
    "    test_df.write.mode(\"overwrite\").parquet(f\"{base_path}/test\")\n",
    "\n",
    "    # Show counts for confirmation\n",
    "    print(f\"Total rows: {total}\")\n",
    "    print(f\"Train rows: {train_df.count()}\")\n",
    "    print(f\"Validation rows: {validation_df.count()}\")\n",
    "    print(f\"Test rows: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33426c58-b2e4-4b55-a9a4-6fa8db8ce61a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No data in sample_df. Creating empty Spark DataFrame with schema.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_text</th><th>bert_features</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "bert_features",
         "type": "{\"type\":\"array\",\"elementType\":\"float\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "\n",
    "# Sample a subset for demonstration (adjust n as needed)\n",
    "sample_df = df.select(\"id\", \"review_text\").limit(1000).toPandas()\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get BERT [CLS] embedding for a text\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding=\"max_length\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use the [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_embedding.astype(np.float32)\n",
    "\n",
    "# Compute BERT embeddings for each review_text\n",
    "sample_df[\"bert_features\"] = sample_df[\"review_text\"].fillna(\"\").apply(get_bert_embedding)\n",
    "\n",
    "bert_pdf = sample_df[[\"id\", \"bert_features\"]]\n",
    "\n",
    "if bert_pdf.empty:\n",
    "    print(\"WARNING: No data in sample_df. Creating empty Spark DataFrame with schema.\")\n",
    "    bert_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"bert_features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    bert_sdf = spark.createDataFrame([], schema=bert_schema)\n",
    "else:\n",
    "    # Convert numpy arrays to lists for Spark compatibility\n",
    "    bert_pdf[\"bert_features\"] = bert_pdf[\"bert_features\"].apply(lambda x: x.tolist())\n",
    "    bert_sdf = spark.createDataFrame(bert_pdf)\n",
    "\n",
    "# Join BERT features back to the original DataFrame (on 'id')\n",
    "result_df = df.join(bert_sdf, on=\"id\", how=\"left\")\n",
    "\n",
    "# Display the DataFrame with BERT features (show a few columns)\n",
    "display(result_df.select(\"review_text\", \"bert_features\"))\n",
    "\n",
    "# NOTE: For large datasets, consider using UDFs or batch processing for scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f953571-220b-478f-b9b6-ee89f6dd1352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/spark-e26db6c5-ed21-4f5e-bbc3-b5/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   reviews  sentiment  ...  bert_766  bert_767\n3  Absolutely fantastic! Highly recommend.     0.8043  ...  0.416754 -0.025454\n0       I love this product! It's amazing.     0.8516  ...  0.066335 -0.029567\n1           Worst purchase I've ever made.    -0.6249  ...  0.225471 -0.151352\n\n[3 rows x 842 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Ensure vader_lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# This is a pandas DataFrame for demonstration. Do NOT overwrite the Spark DataFrame 'df'.\n",
    "data = {\n",
    "    'reviews': [\n",
    "        \"I love this product! It's amazing.\",\n",
    "        \"Worst purchase I've ever made.\",\n",
    "        \"It's okay, not great but not terrible.\",\n",
    "        \"Absolutely fantastic! Highly recommend.\",\n",
    "        \"Do not waste your money on this.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "# Function to calculate sentiment\n",
    "def calculate_sentiment(reviews):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return [sia.polarity_scores(review)['compound'] for review in reviews]\n",
    "\n",
    "# Function to calculate review length\n",
    "def review_length(reviews):\n",
    "    return [len(review.split()) for review in reviews]\n",
    "\n",
    "# Function to extract BERT embeddings\n",
    "def get_bert_embeddings(reviews):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer(reviews, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(pdf['reviews']).toarray()\n",
    "\n",
    "# N-grams\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "ngram_features = count_vectorizer.fit_transform(pdf['reviews']).toarray()\n",
    "\n",
    "# Assemble features into a DataFrame\n",
    "pdf['sentiment'] = calculate_sentiment(pdf['reviews'])\n",
    "pdf['review_length'] = review_length(pdf['reviews'])\n",
    "bert_embeddings = get_bert_embeddings(pdf['reviews'].tolist())\n",
    "bert_df = pd.DataFrame(bert_embeddings, columns=[f'bert_{i}' for i in range(bert_embeddings.shape[1])])\n",
    "\n",
    "# Combine all features\n",
    "features_df = pd.concat([pdf, pd.DataFrame(tfidf_features), pd.DataFrame(ngram_features), bert_df], axis=1)\n",
    "\n",
    "# Display a sample for modeling\n",
    "print(features_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9226a805-a3ad-4b3a-8cb3-3e52804a12fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not read Parquet path abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/feature_v2/train. Creating empty DataFrame with schema. Error: [NOT_IMPLEMENTED] rdd is not implemented.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_text</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define the schema (should match Cell 5)\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"review_text\", StringType(), True),\n",
    "    StructField(\"rating\", IntegerType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"book_id\", StringType(), True),\n",
    "    StructField(\"bert_features\", StringType(), True),\n",
    "    # Add more fields as needed\n",
    "])\n",
    "\n",
    "# 1. Set up Azure Data Lake Storage access (if not already set)\n",
    "account_name = \"goodreadsreviews60313569\"\n",
    "account_key = \"TLLUqPCW94aCOPP/8ic0S/0oWOEa9ccZ/rfHpUw71lh9c2vsCCl2oFGaWifNoAmtRTC45MDRTn0B+AStXqUGDQ==\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{account_name}.dfs.core.windows.net\",\n",
    "    account_key\n",
    ")\n",
    "\n",
    "# 2. Define the path to the Gold dataset (feature_v2/train)\n",
    "parquet_path = (\n",
    "    f\"abfss://lakehouse@{account_name}.dfs.core.windows.net/gold/feature_v2/train\"\n",
    ")\n",
    "\n",
    "# 3. Load the dataset into a Spark DataFrame\n",
    "try:\n",
    "    df = spark.read.schema(schema).parquet(parquet_path)\n",
    "    if df.rdd.isEmpty():\n",
    "        print(f\"WARNING: Parquet path {parquet_path} is empty. Creating empty DataFrame with schema.\")\n",
    "        df = spark.createDataFrame([], schema)\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Could not read Parquet path {parquet_path}. Creating empty DataFrame with schema. Error: {e}\")\n",
    "    df = spark.createDataFrame([], schema)\n",
    "\n",
    "# 4. Display a sample of the review_text column\n",
    "display(df.select(\"review_text\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fb7ee2-2f35-4f62-9c04-1e245a9ef90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c417ed39-5afb-4a92-a4b8-578e20d4e816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    review_text  amazing  ...  worst  worst experience\n1  I did not like this product.      0.0  ...    0.0               0.0\n\n[1 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'review_text': [\n",
    "        'This product is great!',\n",
    "        'I did not like this product.',\n",
    "        'Amazing quality and service.',\n",
    "        'Worst experience ever.',\n",
    "        'I would recommend this to everyone.'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the review_text column\n",
    "tfidf_matrix = vectorizer.fit_transform(df['review_text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the original DataFrame with the TF-IDF features\n",
    "result_df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "# Display a sample of the resulting DataFrame\n",
    "print(result_df.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deb2f032-2f4b-4f69-ac5d-5658566f126f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    review_text  ...  sentiment_compound\n0          I love this product! It works great.  ...              0.8622\n2   It is okay, not the best but not the worst.  ...              0.5729\n3    Absolutely fantastic! Highly recommend it.  ...              0.8043\n4           I am disappointed with the quality.  ...             -0.4767\n1  This is the worst purchase I have ever made.  ...             -0.6249\n\n[5 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'review_text': [\n",
    "        'I love this product! It works great.',\n",
    "        'This is the worst purchase I have ever made.',\n",
    "        'It is okay, not the best but not the worst.',\n",
    "        'Absolutely fantastic! Highly recommend it.',\n",
    "        'I am disappointed with the quality.'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize VADER SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to extract sentiment features\n",
    "def extract_sentiment_features(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return pd.Series({\n",
    "        'sentiment_pos': sentiment['pos'],\n",
    "        'sentiment_neg': sentiment['neg'],\n",
    "        'sentiment_neu': sentiment['neu'],\n",
    "        'sentiment_compound': sentiment['compound']\n",
    "    })\n",
    "\n",
    "# Apply the function to the review_text column\n",
    "sentiment_features = df['review_text'].apply(extract_sentiment_features)\n",
    "\n",
    "# Concatenate the sentiment features to the original DataFrame\n",
    "df = pd.concat([df, sentiment_features], axis=1)\n",
    "\n",
    "# Display a sample of the DataFrame with the new sentiment features\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ec4604d-7793-4265-996d-6c6e2496d8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_text</th><th>review_length_words</th><th>review_length_chars</th></tr></thead><tbody><tr><td>This is a great product.</td><td>5</td><td>24</td></tr><tr><td>Not what I expected.</td><td>4</td><td>20</td></tr><tr><td>Absolutely fantastic!</td><td>2</td><td>21</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "This is a great product.",
         5,
         24
        ],
        [
         "Not what I expected.",
         4,
         20
        ],
        [
         "Absolutely fantastic!",
         2,
         21
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_length_words",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "review_length_chars",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, length, split, size\n",
    "\n",
    "# Use the existing df loaded earlier (from the Gold layer)\n",
    "# If you want to test with a sample DataFrame, uncomment the following lines:\n",
    "data = [(\"This is a great product.\",), (\"Not what I expected.\",), (\"Absolutely fantastic!\",)]\n",
    "df = spark.createDataFrame(data, [\"review_text\"])\n",
    "\n",
    "# Extracting basic text features\n",
    "df = df.withColumn(\"review_length_words\", size(split(col(\"review_text\"), \" \"))).withColumn(\"review_length_chars\", length(col(\"review_text\")))\n",
    "\n",
    "# Display a sample of the resulting DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1426ba1-0fcd-4049-914b-c267c0c10d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  review_text                          cleaned_review\n2      Great value for money. 100% satisfied.  great value for money NUMBER satisfied\n1  Terrible service... would not recommend!!!    terrible service would not recommend\n5              This is a    great product!                    this is a great product\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# This is a pandas DataFrame for demonstration. Do NOT overwrite the Spark DataFrame 'df'.\n",
    "data = {\n",
    "    'review_text': [\n",
    "        \"I love this product! \uD83D\uDE0A Check it out at https://example.com\",\n",
    "        \"Terrible service... would not recommend!!!\",\n",
    "        \"Great value for money. 100% satisfied.\",\n",
    "        \"\uD83D\uDE21 I am so angry!!!\",\n",
    "        \"Short\",\n",
    "        \"   This is a    great product!   \"\n",
    "    ]\n",
    "}\n",
    "\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "def clean_review(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Replace URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', 'URL', text, flags=re.MULTILINE)\n",
    "    # Replace numbers\n",
    "    text = re.sub(r'\\d+', 'NUMBER', text)\n",
    "    # Replace emojis\n",
    "    text = emoji.replace_emoji(text, replace='EMOJI')\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "pdf['cleaned_review'] = pdf['review_text'].apply(clean_review)\n",
    "\n",
    "# Filter out reviews with less than 10 characters\n",
    "pdf_filtered = pdf[pdf['cleaned_review'].str.len() >= 10]\n",
    "\n",
    "# Display a sample of the cleaned reviews\n",
    "print(pdf_filtered[['review_text', 'cleaned_review']].sample(n=3, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa9a5594-2469-4fac-b868-7c8d64acf7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS hive_metastore.default.semantic_embeddings (\n",
    "  bert_embedding ARRAY<DOUBLE> COMMENT 'Contextual vector representation for each review capturing meaning beyond surface words.'\n",
    ");\n",
    "\n",
    "CREATE OR REPLACE VIEW hive_metastore.default.semantic_embedding_features_metric_view AS\n",
    "SELECT\n",
    "  bert_embedding\n",
    "FROM\n",
    "  hive_metastore.default.semantic_embeddings;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb4c2b6-bf63-4bc2-9888-294cb71495ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Index(['review_text', 'amazing', 'amazing quality', 'did', 'did like',\n",
       "       'experience', 'great', 'like', 'like product', 'product',\n",
       "       'product great', 'quality', 'quality service', 'recommend', 'service',\n",
       "       'worst', 'worst experience'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_text</th></tr></thead><tbody><tr><td>This product is great!</td></tr><tr><td>I did not like this product.</td></tr><tr><td>Amazing quality and service.</td></tr><tr><td>Worst experience ever.</td></tr><tr><td>I would recommend this to everyone.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "This product is great!"
        ],
        [
         "I did not like this product."
        ],
        [
         "Amazing quality and service."
        ],
        [
         "Worst experience ever."
        ],
        [
         "I would recommend this to everyone."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the available columns\n",
    "display(result_df.columns)\n",
    "\n",
    "# Display the first 5 rows with additional features if present\n",
    "cols_to_show = [\n",
    "    'review_text',\n",
    "    'review_length_words',\n",
    "    'review_length_chars',\n",
    "    'sentiment_pos',\n",
    "    'sentiment_neg',\n",
    "    'sentiment_neu',\n",
    "    'sentiment_compound'\n",
    "]\n",
    "display(\n",
    "    result_df[\n",
    "        [col for col in cols_to_show if col in result_df.columns]\n",
    "    ].head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae177aae-cf68-4a5c-a0c0-c6e8ea21a347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>review_text</th><th>review_length_words</th><th>review_length_chars</th><th>sentiment_pos</th><th>sentiment_neg</th><th>sentiment_neu</th><th>sentiment_compound</th></tr></thead><tbody><tr><td>This is a great product.</td><td>5</td><td>24</td><td>0.577</td><td>0.0</td><td>0.423</td><td>0.6249</td></tr><tr><td>Not what I expected.</td><td>4</td><td>20</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td></tr><tr><td>Absolutely fantastic!</td><td>2</td><td>21</td><td>0.807</td><td>0.0</td><td>0.193</td><td>0.6352</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "This is a great product.",
         5,
         24,
         0.577,
         0.0,
         0.423,
         0.6249
        ],
        [
         "Not what I expected.",
         4,
         20,
         0.0,
         0.0,
         1.0,
         0.0
        ],
        [
         "Absolutely fantastic!",
         2,
         21,
         0.807,
         0.0,
         0.193,
         0.6352
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "review_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "review_length_words",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "review_length_chars",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_pos",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_neg",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_neu",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_compound",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Broadcast the analyzer for efficiency\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def get_sentiment_scores(text):\n",
    "    scores = sia.polarity_scores(text if text else \"\")\n",
    "    return (float(scores['pos']), float(scores['neg']), float(scores['neu']), float(scores['compound']))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sentiment_pos\", DoubleType(), True),\n",
    "    StructField(\"sentiment_neg\", DoubleType(), True),\n",
    "    StructField(\"sentiment_neu\", DoubleType(), True),\n",
    "    StructField(\"sentiment_compound\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "sentiment_udf = udf(get_sentiment_scores, schema)\n",
    "\n",
    "# Add sentiment columns\n",
    "df = df.withColumn(\"sentiment\", sentiment_udf(df[\"review_text\"]))\n",
    "df = df.withColumn(\"sentiment_pos\", df[\"sentiment\"].sentiment_pos) \\\n",
    "       .withColumn(\"sentiment_neg\", df[\"sentiment\"].sentiment_neg) \\\n",
    "       .withColumn(\"sentiment_neu\", df[\"sentiment\"].sentiment_neu) \\\n",
    "       .withColumn(\"sentiment_compound\", df[\"sentiment\"].sentiment_compound) \\\n",
    "       .drop(\"sentiment\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646467b9-ed55-4840-9df3-d82ad1f77ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "final_df = df.withColumn(\n",
    "    \"features\",\n",
    "    array(\n",
    "        \"review_length_words\",\n",
    "        \"review_length_chars\",\n",
    "        \"sentiment_pos\",\n",
    "        \"sentiment_neg\",\n",
    "        \"sentiment_neu\",\n",
    "        \"sentiment_compound\"\n",
    "    )\n",
    ")\n",
    "\n",
    "final_features_df = final_df.select(\"features\")\n",
    "\n",
    "final_features_df.write.mode(\"overwrite\").parquet(\n",
    "    \"abfss://lakehouse@goodreadsreviews60313569.dfs.core.windows.net/gold/features_v2/\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5960083838370744,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "LAB4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}